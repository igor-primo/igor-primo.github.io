<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="pt" lang="pt">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>Igor's Minimal Presence - Configurações Manuais em Kind</title>
        <link rel="stylesheet" type="text/css" href="../../css/default.css" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
    </head>
    <body class="sans">
        <div id="header">
            <div id="logo">
                <a href="../../pt">Presença Mínima</a>
                <a href="../../pt/contact.html">Vamos conversar</a>
            </div>
        </div>

        <div id="content">
            <article>
                <h1 id="tufte-css">Configurações Manuais em Kind</h1>

<div class="info">
    
        <p class="subtitle">Para quando deixar um cluster de teste ativo é um luxo</p>
    
    04/12/2024
    
</div>

<a href="../../en/posts/2024-12-04-quick-persistence-for-kubernetes-testing.html">English version</a>

<section>
<h2 id="o-problema">O problema</h2>
<p>Às vezes, simplesmente podar um arquivo <code>values.yaml</code> para remover aplicações
desnecessárias do seu cluster K8s local implantado com Kind não é suficiente
para um fluxo de aprendizado agradável. Em uma máquina com recursos restritos,
deixar um cluster de teste local ativo enquanto se faz outras tarefas de
desenvolvimento, enquanto se usa um navegador de internet ou simplesmente se brinca no
computador, não é suportável. Isso gera um problema para configurações manuais
aplicadas enquanto se testam aplicações, como criar usuários na sua aplicação de
forma dinâmica, que serão destruídos ao desmontar o cluster.</p>
<p>Em circunstâncias como essas, configurar serviços externos
<label for="sn-heard-production" class="margin-toggle sidenote-number">
</label>
<input type="checkbox" id="sn-heard-production" class="margin-toggle" />
<span class="sidenote">
NFS, iSCSI, SAN, etc.
</span>
para o cluster de modo a manter os dados por meio de Persistent Volumes
<label for="sn-pv-definition" class="margin-toggle sidenote-number">
</label>
<input type="checkbox" id="sn-pv-definition" class="margin-toggle" />
<span class="sidenote">
Persistent Volumes são recursos consumidos por Persistent Volume Claims, da
mesma forma que os recursos dos nós (nodes) são usados pelos pods. Os recursos básicos
são o tamanho de armazenamento e seus modos de acesso.
</span>
(PVs) pode não ser suficiente, por razões semelhantes. Vamos investigar uma
solução alternativa, mais leve.</p>
</section>
<section>
<h2 id="compreendendo-a-situação">Compreendendo a situação</h2>
<p>Segue uma descrição da situação. Usamos o Kind para orquestrar containers em
cima de 3 containers, cada um dos quais desempenha o papel de um nó em nosso
cluster, executado pelo Docker. O Kind instala os componentes do Kubernetes
nesses 3 containers.</p>
<p>Ao criar os pods que declaramos, também são criados os Persistent Volume Claims
(PVCs), que solicitam seus respectivos PVs dinamicamente criados. Esses PVCs ficam
em um diretório dentro dos nós workers (<code>/var/local-path-provisioner</code>), onde os
respectivos pods residem. O problema agora parece um pouco mais claro.
Containers Docker são efêmeros. Se for esse o caso, esses PVCs não
sobreviverão ao se derrubar o cluster.</p>
<p>Vamos supor que nossa configuração do Kind seja escrita assim:</p>
<pre>
<code>
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
- role: worker
- role: worker
containerdConfigPatches:
  - |
    [plugins."io.containerd.grpc.v1.cri."registry.mirrors."harbor.localhost.com"]
      endpoint = ["https://harbor.localhost.com"]
    [plugins."io.containerd.grpc.v1.cri."registry.configs]
      [plugins."io.containerd.grpc.v1.cri."registry.configs."harbor.localhost.com".tls]
        insecure_skip_verify = true

    
</code>
</pre>
<p>Suponha
<label for="mn-demo" class="margin-toggle">
∮
</label>
<input type="checkbox" id="mn-demo" class="margin-toggle" />
<span class="marginnote">
Essas configurações foram retiradas de um projeto de autoaprendizado que eu fiz
para aprender GitFlow. É bastante completo. Envolve a configuração de um
registro de imagens de container, um software de controle de versão de
código-fonte e o Jenkins. Isso explica a configuração do Harbor no localhost, e
o TLS skip que você nunca deve fazer em produção.
</span>
que estamos implantando uma instância do Gitea, configurada assim no nosso <code>helmfile.yaml</code>:</p>
<pre>
<code>
...

- name: gitea
  namespace: gitea
  createNamespace: true
  chart: gitea/gitea
  version: 10.1.1
  values:
    - values/gitea/values.yaml

...

</code>
</pre>
<p>Podemos esperar até ter uma instância executando…</p>
<pre>
<code>
% watch kubectl get pod -n gitea
...
NAME                                          READY   STATUS    RESTARTS   AGE
gitea-5fd6ccd9f9-4hmvp                        1/1     Running   0          2m22s
gitea-postgresql-ha-pgpool-6485454559-spj6d   1/1     Running   0          2m22s
gitea-postgresql-ha-postgresql-0              1/1     Running   0          2m22s
gitea-postgresql-ha-postgresql-1              1/1     Running   0          2m22s
gitea-postgresql-ha-postgresql-2              1/1     Running   0          2m22s

</code>
</pre>
<p>…e então, inspecionando os nós workers da seguinte maneira…</p>
<pre>
<code>
% docker ps --format=json | jq '[.Names, .ID]'
...
[
  "kind-control-plane",
  "0d71f246fd69"
]
[
  "kind-worker",
  "2cd7e78b696a"
]
[
  "kind-worker2",
  "9b4c172d8b13"
]
...
% docker exec -it 2cd sh -c 'ls /var/local-path-provisioner'
...
pvc-649ac59a-5c7b-4b00-9e37-d99d44571ab3_gitea_data-gitea-postgresql-ha-postgresql-2
pvc-90a3f53a-b9f4-4ea1-85a1-7b19b9fa0ab1_gitea_data-gitea-postgresql-ha-postgresql-0
...
% docker exec -it 9b4 sh -c 'ls /var/local-path-provisioner'
...
pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1
pvc-a247f26e-26b5-4ec5-9d3b-6b51569319ab_gitea_gitea-shared-storage

</code>
</pre>
<p>…pegamos esses verminhos preparando uma traquinagem.</p>
<p>Esses são os PVCs onde os dados que os containers no pod utilizam estão
armazenados. Na verdade, podemos verificar isso um pouco mais a fundo
executando um comando como…</p>
<pre>
<code>
% docker exec -it 9b4 sh -c 'apt update -y && apt install -y file && find /var/local-path-provisioner/ -type f -exec file '{}' \;'
...
/var/local-path-provisioner/pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1/data/postmaster.pid: ASCII text
/var/local-path-provisioner/pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1/data/backup_label.old: ASCII text
/var/local-path-provisioner/pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1/data/postmaster.opts: ASCII text
/var/local-path-provisioner/pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1/data/pg_xact/0000: International EBCDIC text, with very long lines (311), with NEL line terminators
/var/local-path-provisioner/pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1/data/pg_ident.conf: ASCII text
/var/local-path-provisioner/pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1/data/PG_VERSION: ASCII text
/var/local-path-provisioner/pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1/data/postgresql.auto.conf: ASCII text
/var/local-path-provisioner/pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1/data/pg_subtrans/0000: data
/var/local-path-provisioner/pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1/data/pg_wal/000000010000000000000006: data
/var/local-path-provisioner/pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1/data/pg_wal/archive_status/000000010000000000000007.done: empty
/var/local-path-provisioner/pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1/data/pg_wal/archive_status/000000010000000000000006.done: empty
/var/local-path-provisioner/pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1/data/pg_wal/000000010000000000000005: DIY-Thermocam raw data (Lepton 3.x), scale 16-134, spot sensor temperature 0.000000, unit celsius, color scheme 0, calibration: offset 0.000000, slope 4.187508
/var/local-path-provisioner/pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1/data/pg_wal/000000010000000000000007: data
/var/local-path-provisioner/pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1/data/pg_wal/000000010000000000000008: data
/var/local-path-provisioner/pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1/data/pg_multixact/members/0000: data
/var/local-path-provisioner/pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1/data/pg_multixact/offsets/0000: data
/var/local-path-provisioner/pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1/data/base/16387/3394_vm: data
/var/local-path-provisioner/pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1/data/base/16387/3575: data
/var/local-path-provisioner/pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1/data/base/16387/2833: data
...

</pre>
<p></code></p>
<p>Agora, vamos desmontar nosso cluster e inspecionar os hashes.</p>
<pre>
<code>
% docker ps --format=json | jq '[.Names, .ID]'
...
[
  "kind-worker2",
  "7dbffcf0e4ea"
]
[
  "kind-worker",
  "f0bb7e95efe6"
]
[
  "kind-control-plane",
  "4f62bd1e35b0"
]
...
% docker exec -it f0bb sh -c 'ls /var/local-path-provisioner'
...
pvc-1e3a7684-8efc-444d-baad-9c3824ecafad_gitea_data-gitea-postgresql-ha-postgresql-0
pvc-9e8d077a-10d8-418d-bbfc-995dc1a4c9df_gitea_gitea-shared-storage

</code>
</pre>
<p>Então, verificamos que os hashes mudaram. Os armazenamentos foram
destruídos e recriados, deixando-nos com a responsabilidade de recriar nossa
configuração manual.</p>
</section>
<section>
<h2 id="a-solução">A solução</h2>
<p>A solução consiste em criar Persistent Volumes e Persistent Volume Claims
estáticos, que garantem sua identificação durante desmontagens do cluster; e
também, a configuração de <code>persistentVolumeReclaimPolicy</code> para <code>Retain</code>
<label for="sn-retain-definition" class="margin-toggle sidenote-number">
</label>
<input type="checkbox" id="sn-retain-definition" class="margin-toggle" />
<span class="sidenote">
As opções para essa configuração são Retain, Delete ou Recycle. A política
Retain permite a recuperação manual do recurso. Quando o PersistentVolumeClaim
é excluído, o PersistentVolume ainda existe e o volume é considerado
“liberado”. Mas ele ainda não está disponível para outra solicitação porque os
dados do reclamante anterior permanecem no volume. A opção Delete é óbvia.
Recycle realiza apenas uma limpeza básica: <code>rm -rf /thevolume/*</code>
</span>
, para que os armazenamentos possam ser reclamados, e com <code>volumeMode</code>
configurado como <code>Filesystem</code>, para que possamos usar o sistema de arquivos do
host para armazenar os PVCs.</p>
<p>Também precisaremos instruir o Kind para montar, em um diretório fornecido pelo
usuário, o diretório do container onde os PVCs estão armazenados. Para isso,
precisaremos criar um diretório:</p>
<pre>
<code>
mkdir .volumes
</code>
</pre>
<p>E, então, modificar <code>config.yaml</code> assim:</p>
<pre>
<code>
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
- role: worker
  extraMounts:
    - hostPath: .volumes
      containerPath: /var/local-path-provisioner
- role: worker
  extraMounts:
    - hostPath: .volumes
      containerPath: /var/local-path-provisioner
</code>
</pre>
<p>No caso dos dados persistidos pelo próprio pod do Gitea, podemos fazer…
<label for="mn-shared-storage" class="margin-toggle">
∮
</label>
<input type="checkbox" id="mn-shared-storage" class="margin-toggle" />
<span class="marginnote">
Aqui, ‘gitea-shared-storage’ é o nome padrão da solicitação de armazenamento
configurado no arquivo de configuração <code>values.yaml</code> do Gitea. Precisamos,
primeiro, criar o namespace <code>gitea</code> e então criar os recursos desejados lá
antes de implantar o Gitea. O manifesto resultante precisa ser aplicado antes
da invocação do Helm, usando algo como <code>kubectl apply -f manifests/gitea-storage.yaml</code>.
</span></p>
<pre>
<code>
export FILE="manifests/gitea-storage.yaml"

echo '---' >> $FILE && \
cat >> $FILE &lt&ltEOF
apiVersion: v1
kind: Namespace
metadata:
  name: gitea
EOF

echo '---' >> $FILE && \
kubectl get pv $(kubectl get pv | grep 'gitea-shared-storage' | awk '{print $1}') -oyaml >> $FILE && \
echo '---' >> $FILE && \
kubectl get pvc -n gitea gitea-shared-storage -oyaml >> $FILE

</code>
</pre>
<p>O arquivo resultante pode parecer com isso:</p>
<pre>
<code>
---
apiVersion: v1
kind: Namespace
metadata:
  name: gitea
---
apiVersion: v1
kind: PersistentVolume
metadata:
  annotations:
    pv.kubernetes.io/provisioned-by: rancher.io/local-path
  creationTimestamp: "2024-12-05T04:50:16Z"
  finalizers:
  - kubernetes.io/pv-protection
  name: pvc-077c94a5-4826-4bbf-8af6-62ff43bf692a
  resourceVersion: "997"
  uid: 5cb33644-41aa-4e21-b56a-0800dd25bc37
spec:
  accessModes:
  - ReadWriteOnce
  capacity:
    storage: 10Gi
  claimRef:
    apiVersion: v1
    kind: PersistentVolumeClaim
    name: gitea-shared-storage
    namespace: gitea
    resourceVersion: "863"
    uid: 077c94a5-4826-4bbf-8af6-62ff43bf692a
  hostPath:
    path: /var/local-path-provisioner/pvc-077c94a5-4826-4bbf-8af6-62ff43bf692a_gitea_gitea-shared-storage
    type: DirectoryOrCreate
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - kind-worker2
  persistentVolumeReclaimPolicy: Delete
  storageClassName: standard
  volumeMode: Filesystem
status:
  lastPhaseTransitionTime: "2024-12-05T04:50:16Z"
  phase: Bound
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  annotations:
    helm.sh/resource-policy: keep
    meta.helm.sh/release-name: gitea
    meta.helm.sh/release-namespace: gitea
    pv.kubernetes.io/bind-completed: "yes"
    pv.kubernetes.io/bound-by-controller: "yes"
    volume.beta.kubernetes.io/storage-provisioner: rancher.io/local-path
    volume.kubernetes.io/selected-node: kind-worker2
    volume.kubernetes.io/storage-provisioner: rancher.io/local-path
  creationTimestamp: "2024-12-05T04:50:08Z"
  finalizers:
  - kubernetes.io/pvc-protection
  labels:
    app.kubernetes.io/managed-by: Helm
  name: gitea-shared-storage
  namespace: gitea
  resourceVersion: "999"
  uid: 077c94a5-4826-4bbf-8af6-62ff43bf692a
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: standard
  volumeMode: Filesystem
  volumeName: pvc-077c94a5-4826-4bbf-8af6-62ff43bf692a
status:
  accessModes:
  - ReadWriteOnce
  capacity:
    storage: 10Gi
  phase: Bound

</code>
</pre>
<p>Que precisa ser podado para ficar mais ou menos assim
<label for="mn-pruned" class="margin-toggle">
∮
</label>
<input type="checkbox" id="mn-pruned" class="margin-toggle" />
<span class="marginnote">
Aqui, removemos rótulos e anotações que são dinamicamente mesclados com a
configuração do pod e servem para fins de satisfazer a lógica da aplicação,
assim como outros dados de observabilidade. Eles não têm utilidade na
declaração da configuração neste momento. Observe as especificações
<code>metadata.name</code>,
<code>spec.hostPath.path</code>,
<code>spec.persistentVolumeReclaimPolicy</code>,
<code>spec.volumeName</code>.
Perceba como são consistentes.
</span>:</p>
<pre>
<code>
---
apiVersion: v1
kind: Namespace
metadata:
  name: gitea
---
apiVersion: v1
kind: PersistentVolume
metadata:
  annotations:
    pv.kubernetes.io/provisioned-by: rancher.io/local-path
  name: pvc-gitea-storage
spec:
  accessModes:
  - ReadWriteOnce
  capacity:
    storage: 10Gi
  claimRef: # claimRef ensures a one-to-one binding.
    apiVersion: v1
    kind: PersistentVolumeClaim
    name: gitea-shared-storage
    namespace: gitea
  hostPath:
    path: /var/local-path-provisioner/pvc-gitea-storage
    type: DirectoryOrCreate
  persistentVolumeReclaimPolicy: Retain
  storageClassName: standard
  volumeMode: Filesystem
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  annotations:
    helm.sh/resource-policy: keep
    meta.helm.sh/release-name: gitea
    meta.helm.sh/release-namespace: gitea
    volume.beta.kubernetes.io/storage-provisioner: rancher.io/local-path
    volume.kubernetes.io/storage-provisioner: rancher.io/local-path
  labels:
    app.kubernetes.io/managed-by: Helm
  name: gitea-shared-storage
  namespace: gitea
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: standard
  volumeMode: Filesystem
  volumeName: pvc-gitea-storage

</code>
</pre>
<p>Aplicar a configuração após a desmontagem do cluster não tem um resultado
imediato. É necessário definir permissões apropriadas no diretório criado
<label for="sn-permissions" class="margin-toggle sidenote-number">
</label>
<input type="checkbox" id="sn-permissions" class="margin-toggle" />
<span class="sidenote">
Alguns arquivos ou diretórios criados nos hosts subjacentes podem ser
acessíveis apenas pelo root. É necessário executar o processo como root em um
container privilegiado ou modificar as permissões de arquivo no host para poder
ler (ou gravar) em um volume do tipo hostPath.
</span>
:</p>
<pre>
<code>
chmod ao+w .volumes/pvc-gitea-storage
</code>
</pre>
<p>E agora eu tenho configurações manuais persistentes.</p>
<p>O repositório respectivo pode ser encontrado
<a href="https://github.com/igor-primo/gitflow">aqui</a>.</p>
</section>

            </article>
        </div>
    </body>
</html>
