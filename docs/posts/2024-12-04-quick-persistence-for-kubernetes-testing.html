<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>Igor's Minimal Presence - Manual Configurations In Kind</title>
        <link rel="stylesheet" type="text/css" href="../css/default.css" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
    </head>
    <body class="sans">
        <div id="header">
            <div id="logo">
                <a href="../">Minimal Presence</a>
                <a href="../contact.html">Talk to me</a>
            </div>
            <!-- <div id="navigation">
                <a href="/">Home</a>
                <a href="/about.html">About</a>
                <a href="/contact.html">Contact</a>
                <a href="/archive.html">Archive</a>
            </div> -->
        </div>

        <div id="content">
            <article>
                <!-- <h1 id="tufte-css">Manual Configurations In Kind</h1> -->
                <h1 id="tufte-css">Manual Configurations In Kind</h1>

<div class="info">
    
        <p class="subtitle">For when leaving a test cluster up is a luxury</p>
    
    December  4, 2024
    
</div>

<section>
<h2 id="the-problem">The problem</h2>
<p>Sometimes, simply pruning a <code>values.yaml</code> file to remove unnecessary applications
from your Kind deployed K8s local cluster is not sufficient for a
straightforward learning flow. On a resource restricted machine, leaving a
local test cluster up while doing other dev chores, using a web browser, or
simply playing around in one’s computer, is not bearable. This poses a
problem for manual configurations applied while testing applications, such as
creating users on the fly in your app, which will be destroyed upon cluster
teardown.</p>
<p>In circumstances like these, setting up services external
<label for="sn-heard-production" class="margin-toggle sidenote-number">
</label>
<input type="checkbox" id="sn-heard-production" class="margin-toggle" />
<span class="sidenote">
NFS, iSCSI, SAN, etc.
</span>
to the cluster to hold the data via Persistent Volumes
<label for="sn-pv-definition" class="margin-toggle sidenote-number">
</label>
<input type="checkbox" id="sn-pv-definition" class="margin-toggle" />
<span class="sidenote">
Persistent Volumes are resources consumed by Persistent Volume Claims, the same way
node resources are used by pods. The basic resources are storage size and its
access modes.
</span>
(PVs) might not do, for similar reasons. Let’s investigate an alternative,
lightweight solution.</p>
</section>
<section>
<h2 id="understanding-the-situation">Understanding the situation</h2>
<p>A description of the situation follows. We use Kind to orchestrate containers on
top of 3 containers, each of which plays the role of a node in our cluster, run
by Docker. Kind installs Kubernetes components in these 3 containers.</p>
<p>Upon creation of the pods we declare, Persistent Volume Claims (PVCs) are also
created, which claim their respective dynamically
<label for="sn-dynamic-pvc" class="margin-toggle sidenote-number">
</label>
<input type="checkbox" id="sn-dynamic-pvc" class="margin-toggle" />
<span class="sidenote">
If none of the existing static PVs match a user’s
PVC, the cluster tries to dynamically provision a volume for the PVC.
</span>
created PVs. These PVCs sit in
a directory inside the worker nodes (<code>/var/local-path-provisioner</code>) where the
respective pods reside
<label for="sn-pv-definition" class="margin-toggle sidenote-number">
</label>
<input type="checkbox" id="sn-pv-definition" class="margin-toggle" />
<span class="sidenote">
From the point of view of a pod, PVCs are volumes!
</span>
. The issue now seems a bit more clear. Docker containers
are ephemeral. If so, these PVCs will not survive cluster teardowns.</p>
<p>Let’s say our Kind configuration is sung like this:</p>
<pre>
<code>
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
- role: worker
- role: worker
containerdConfigPatches:
  - |
    [plugins."io.containerd.grpc.v1.cri."registry.mirrors."harbor.localhost.com"]
      endpoint = ["https://harbor.localhost.com"]
    [plugins."io.containerd.grpc.v1.cri."registry.configs]
      [plugins."io.containerd.grpc.v1.cri."registry.configs."harbor.localhost.com".tls]
        insecure_skip_verify = true

    
</code>
</pre>
<p>Suppose
<label for="mn-demo" class="margin-toggle">
∮
</label>
<input type="checkbox" id="mn-demo" class="margin-toggle" />
<span class="marginnote">
These configurations are taken from a self-learning hobby project I made to
learn GitFlow. It is quite complete. Involves setting up a container image
registry, a source version control software and Jenkins. This explains the
localhost Harbor configuration, and the TLS skip you should never do in
production.
</span>
we are releasing a Gitea instance, configured like this in our <code>helmfile.yaml</code>:</p>
<pre>
<code>
...

- name: gitea
  namespace: gitea
  createNamespace: true
  chart: gitea/gitea
  version: 10.1.1
  values:
    - values/gitea/values.yaml

...

</code>
</pre>
<p>We can wait until we have a working instance by running…</p>
<pre>
<code>
% watch kubectl get pod -n gitea
...
NAME                                          READY   STATUS    RESTARTS   AGE
gitea-5fd6ccd9f9-4hmvp                        1/1     Running   0          2m22s
gitea-postgresql-ha-pgpool-6485454559-spj6d   1/1     Running   0          2m22s
gitea-postgresql-ha-postgresql-0              1/1     Running   0          2m22s
gitea-postgresql-ha-postgresql-1              1/1     Running   0          2m22s
gitea-postgresql-ha-postgresql-2              1/1     Running   0          2m22s

</code>
</pre>
<p>…and then, inspecting the worker nodes the following way…</p>
<pre>
<code>
% docker ps --format=json | jq '[.Names, .ID]'
...
[
  "kind-control-plane",
  "0d71f246fd69"
]
[
  "kind-worker",
  "2cd7e78b696a"
]
[
  "kind-worker2",
  "9b4c172d8b13"
]
...
% docker exec -it 2cd sh -c 'ls /var/local-path-provisioner'
...
pvc-649ac59a-5c7b-4b00-9e37-d99d44571ab3_gitea_data-gitea-postgresql-ha-postgresql-2
pvc-90a3f53a-b9f4-4ea1-85a1-7b19b9fa0ab1_gitea_data-gitea-postgresql-ha-postgresql-0
...
% docker exec -it 9b4 sh -c 'ls /var/local-path-provisioner'
...
pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1
pvc-a247f26e-26b5-4ec5-9d3b-6b51569319ab_gitea_gitea-shared-storage

</code>
</pre>
<p>…we catch these little worms preparing some trouble.</p>
<p>These are the PVCs where the data the containers in the pod use. In fact, we
can verify this a little further by running an incantation like…</p>
<pre>
<code>
% docker exec -it 9b4 sh -c 'apt update -y && apt install -y file && find /var/local-path-provisioner/ -type f -exec file '{}' \;'
...
/var/local-path-provisioner/pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1/data/postmaster.pid: ASCII text
/var/local-path-provisioner/pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1/data/backup_label.old: ASCII text
/var/local-path-provisioner/pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1/data/postmaster.opts: ASCII text
/var/local-path-provisioner/pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1/data/pg_xact/0000: International EBCDIC text, with very long lines (311), with NEL line terminators
/var/local-path-provisioner/pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1/data/pg_ident.conf: ASCII text
/var/local-path-provisioner/pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1/data/PG_VERSION: ASCII text
/var/local-path-provisioner/pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1/data/postgresql.auto.conf: ASCII text
/var/local-path-provisioner/pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1/data/pg_subtrans/0000: data
/var/local-path-provisioner/pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1/data/pg_wal/000000010000000000000006: data
/var/local-path-provisioner/pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1/data/pg_wal/archive_status/000000010000000000000007.done: empty
/var/local-path-provisioner/pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1/data/pg_wal/archive_status/000000010000000000000006.done: empty
/var/local-path-provisioner/pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1/data/pg_wal/000000010000000000000005: DIY-Thermocam raw data (Lepton 3.x), scale 16-134, spot sensor temperature 0.000000, unit celsius, color scheme 0, calibration: offset 0.000000, slope 4.187508
/var/local-path-provisioner/pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1/data/pg_wal/000000010000000000000007: data
/var/local-path-provisioner/pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1/data/pg_wal/000000010000000000000008: data
/var/local-path-provisioner/pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1/data/pg_multixact/members/0000: data
/var/local-path-provisioner/pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1/data/pg_multixact/offsets/0000: data
/var/local-path-provisioner/pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1/data/base/16387/3394_vm: data
/var/local-path-provisioner/pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1/data/base/16387/3575: data
/var/local-path-provisioner/pvc-1a98a042-7ce9-4c89-b588-5edb8f16c1b2_gitea_data-gitea-postgresql-ha-postgresql-1/data/base/16387/2833: data
...

</pre>
<p></code></p>
<p>Now, let’s teardown our cluster and inspect the hashes.</p>
<pre>
<code>
% docker ps --format=json | jq '[.Names, .ID]'
...
[
  "kind-worker2",
  "7dbffcf0e4ea"
]
[
  "kind-worker",
  "f0bb7e95efe6"
]
[
  "kind-control-plane",
  "4f62bd1e35b0"
]
...
% docker exec -it f0bb sh -c 'ls /var/local-path-provisioner'
...
pvc-1e3a7684-8efc-444d-baad-9c3824ecafad_gitea_data-gitea-postgresql-ha-postgresql-0
pvc-9e8d077a-10d8-418d-bbfc-995dc1a4c9df_gitea_gitea-shared-storage

</code>
</pre>
<p>An then, we can verify that the hashes changed. The storages were destroyed and
recreated, leaving us with the responsibility to recreate our manual
configuration.</p>
</section>
<section>
<h2 id="the-solution">The solution</h2>
<p>The solution consists in creating static Persistent Volumes and static
Persistent Volume Claims, which guarantee their identification across cluster
teardowns; and also, the configuration of <code>persistentVolumeReclaimPolicy</code> to
<code>Retain</code>
<label for="sn-retain-definition" class="margin-toggle sidenote-number">
</label>
<input type="checkbox" id="sn-retain-definition" class="margin-toggle" />
<span class="sidenote">
The options for this configuration are Retain, Delete or Recycle. The Retain
policy allows for manual reclamation of the resource. When the
PersistentVolumeClaim is deleted, the PersistentVolume still exists and the
volume is considered “released”. But it is not yet available for another claim
because the previous claimant’s data remains on the volume. Delete option is
obvious. Recycle just performs a basic scrub: rm -rf /thevolume/*
</span>
, so that the storages can be reclaimed and of <code>volumeMode</code> to
<code>Filesystem</code>, so that we can use the host’s filesystem to store the PVCs.</p>
<p>We will also need to instruct Kind to mount, in a user provided directory,
the container directory where the PVCs are stored. For this we will need to create a directory:</p>
<pre>
<code>
mkdir .volumes
</code>
</pre>
<p>And, then, to modify <code>config.yaml</code> like so:</p>
<pre>
<code>
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
- role: worker
  extraMounts:
    - hostPath: .volumes
      containerPath: /var/local-path-provisioner
- role: worker
  extraMounts:
    - hostPath: .volumes
      containerPath: /var/local-path-provisioner
</code>
</pre>
<p>For the case of data persisted by the very Gitea pod, we can do…
<label for="mn-shared-storage" class="margin-toggle">
∮
</label>
<input type="checkbox" id="mn-shared-storage" class="margin-toggle" />
<span class="marginnote">
Here, ‘gitea-shared-storage’ is the default storage claim name configured in its
values.yaml configuration file.
We need, first, to create the <code>gitea</code> namespace and then create the desired
resources there before deploying Gitea. The resulting manifest needs to be
applied before helm invocation, using something like <code>kubectl apply -f manifests/gitea-storage.yaml</code>.
</span></p>
<pre>
<code>
export FILE="manifests/gitea-storage.yaml"

echo '---' >> $FILE && \
cat >> $FILE &lt&ltEOF
apiVersion: v1
kind: Namespace
metadata:
  name: gitea
EOF

echo '---' >> $FILE && \
kubectl get pv $(kubectl get pv | grep 'gitea-shared-storage' | awk '{print $1}') -oyaml >> $FILE && \
echo '---' >> $FILE && \
kubectl get pvc -n gitea gitea-shared-storage -oyaml >> $FILE

</code>
</pre>
<p>The resulting file might look like this:</p>
<pre>
<code>
---
apiVersion: v1
kind: Namespace
metadata:
  name: gitea
---
apiVersion: v1
kind: PersistentVolume
metadata:
  annotations:
    pv.kubernetes.io/provisioned-by: rancher.io/local-path
  creationTimestamp: "2024-12-05T04:50:16Z"
  finalizers:
  - kubernetes.io/pv-protection
  name: pvc-077c94a5-4826-4bbf-8af6-62ff43bf692a
  resourceVersion: "997"
  uid: 5cb33644-41aa-4e21-b56a-0800dd25bc37
spec:
  accessModes:
  - ReadWriteOnce
  capacity:
    storage: 10Gi
  claimRef:
    apiVersion: v1
    kind: PersistentVolumeClaim
    name: gitea-shared-storage
    namespace: gitea
    resourceVersion: "863"
    uid: 077c94a5-4826-4bbf-8af6-62ff43bf692a
  hostPath:
    path: /var/local-path-provisioner/pvc-077c94a5-4826-4bbf-8af6-62ff43bf692a_gitea_gitea-shared-storage
    type: DirectoryOrCreate
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - kind-worker2
  persistentVolumeReclaimPolicy: Delete
  storageClassName: standard
  volumeMode: Filesystem
status:
  lastPhaseTransitionTime: "2024-12-05T04:50:16Z"
  phase: Bound
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  annotations:
    helm.sh/resource-policy: keep
    meta.helm.sh/release-name: gitea
    meta.helm.sh/release-namespace: gitea
    pv.kubernetes.io/bind-completed: "yes"
    pv.kubernetes.io/bound-by-controller: "yes"
    volume.beta.kubernetes.io/storage-provisioner: rancher.io/local-path
    volume.kubernetes.io/selected-node: kind-worker2
    volume.kubernetes.io/storage-provisioner: rancher.io/local-path
  creationTimestamp: "2024-12-05T04:50:08Z"
  finalizers:
  - kubernetes.io/pvc-protection
  labels:
    app.kubernetes.io/managed-by: Helm
  name: gitea-shared-storage
  namespace: gitea
  resourceVersion: "999"
  uid: 077c94a5-4826-4bbf-8af6-62ff43bf692a
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: standard
  volumeMode: Filesystem
  volumeName: pvc-077c94a5-4826-4bbf-8af6-62ff43bf692a
status:
  accessModes:
  - ReadWriteOnce
  capacity:
    storage: 10Gi
  phase: Bound

</code>
</pre>
<p>Which needs to be pruned so as to look like
<label for="mn-pruned" class="margin-toggle">
∮
</label>
<input type="checkbox" id="mn-pruned" class="margin-toggle" />
<span class="marginnote">
Here, we remove labels and annotations that are dynamically merged with the
pod’s configuration and serve purposes of satistfying the application logic, as
well as other observability data. They have no use in configuration declaration.
Notice the specs
<code>metadata.name</code>,
<code>spec.hostPath.path</code>,
<code>spec.persistentVolumeReclaimPolicy</code>,
<code>spec.volumeName</code>.
Notice how they are consistent.
</span>:</p>
<pre>
<code>
---
apiVersion: v1
kind: Namespace
metadata:
  name: gitea
---
apiVersion: v1
kind: PersistentVolume
metadata:
  annotations:
    pv.kubernetes.io/provisioned-by: rancher.io/local-path
  name: pvc-gitea-storage
spec:
  accessModes:
  - ReadWriteOnce
  capacity:
    storage: 10Gi
  claimRef: # claimRef ensures a one-to-one binding.
    apiVersion: v1
    kind: PersistentVolumeClaim
    name: gitea-shared-storage
    namespace: gitea
  hostPath:
    path: /var/local-path-provisioner/pvc-gitea-storage
    type: DirectoryOrCreate
  persistentVolumeReclaimPolicy: Retain
  storageClassName: standard
  volumeMode: Filesystem
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  annotations:
    helm.sh/resource-policy: keep
    meta.helm.sh/release-name: gitea
    meta.helm.sh/release-namespace: gitea
    volume.beta.kubernetes.io/storage-provisioner: rancher.io/local-path
    volume.kubernetes.io/storage-provisioner: rancher.io/local-path
  labels:
    app.kubernetes.io/managed-by: Helm
  name: gitea-shared-storage
  namespace: gitea
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: standard
  volumeMode: Filesystem
  volumeName: pvc-gitea-storage

</code>
</pre>
<p>Applying the config after a cluster teardown does not have an immediate result.
It is necessary to set appropriate permissions on the created directory
<label for="sn-permissions" class="margin-toggle sidenote-number">
</label>
<input type="checkbox" id="sn-permissions" class="margin-toggle" />
<span class="sidenote">
Some files or directories created on the underlying hosts might only be
accessible by root. It is necessary to run the process as root in a
privileged container or modify the file permissions on the host to be able to
read from (or write to) a hostPath volume.
</span>
:</p>
<pre>
<code>
chmod ao+w .volumes/pvc-gitea-storage
</code>
</pre>
<p>And, now I have persistent manual configurations.</p>
<p>The respective repository can be found <a href="https://github.com/igor-primo/gitflow">here</a>.</p>
</section>

            </article>
        </div>
    </body>
</html>
